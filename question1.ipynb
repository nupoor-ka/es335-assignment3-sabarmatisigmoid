{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LINK TO STREAMLIT APP"
      ],
      "metadata": {
        "id": "fB4OmGagGCBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://es335-assignment3-sabarmatisigmoid-boz6q2bg66xgu3rc4dcwmz.streamlit.app/"
      ],
      "metadata": {
        "id": "anqiQJ8BGFs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING RELEVANT LIBRARIES AND DATA"
      ],
      "metadata": {
        "id": "lQteQRbR5g8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tSLgR1JOMxTe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "import re # for regular expressions\n",
        "from sklearn.manifold import TSNE # for t-SNE\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina' # to make graphs sharper\n",
        "from pprint import pprint # pretty print"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # choosing device to run with"
      ],
      "metadata": {
        "id": "paAii7JhbTXr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device # checking which device is being used"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an1b-uzbbW64",
        "outputId": "d18d7283-cad3-4192-a69d-52b63673e09c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/War_and_Peace/'"
      ],
      "metadata": {
        "id": "h-ybBYjcpL5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aab6eeb-0c9e-4e30-e09a-46322797d606"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(folder_path + 'processed_warandpeace.txt', 'r') as f: # loading processed text\n",
        "    pr_text = json.load(f)\n",
        "\n",
        "with open(folder_path + 'word2int.pkl', 'rb') as f: # loading word2int dictionary\n",
        "    word2int = pickle.load(f)\n",
        "\n",
        "with open(folder_path + 'int2word.pkl', 'rb') as f: # loading int2word dictionary\n",
        "    int2word = pickle.load(f)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G7n_LEayvOth"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=list(word2int.keys())\n",
        "num_words=len(pr_text.split())\n",
        "vocab_size = len(words)"
      ],
      "metadata": {
        "id": "maGSeSlj3NaL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words[:10]+words[18326:18336]:\n",
        "  print(word2int[word], ':', word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3uGaetqEN32",
        "outputId": "f2ac913c-af10-4829-ac39-bec540e2a894"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :  \n",
            "1 : .\n",
            "2 : .2\n",
            "3 : .ar\n",
            "4 : .ate\n",
            "5 : .ly\n",
            "6 : .n\n",
            "7 : .not\n",
            "8 : .o\n",
            "9 : .of\n",
            "18326 : znamenka\n",
            "18327 : zone\n",
            "18328 : zoology\n",
            "18329 : zu\n",
            "18330 : zubov\n",
            "18331 : zubova\n",
            "18332 : zubovski\n",
            "18333 : zum\n",
            "18334 : zweck\n",
            "18335 : <UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__XHz3FaD6V5",
        "outputId": "4804cdff-61ac-4a3c-aa8b-1ba07c74c070"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "593963\n",
            "18336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_word_len = 0\n",
        "for word in words:\n",
        "  if len(word)>max_word_len:\n",
        "    max_word_len = len(word)\n",
        "print(max_word_len) # length of longest word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQTsOiWO8eJO",
        "outputId": "0dbf5ce5-2069-4a93-cecb-fc0dcaf2c71c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line_len = len(\"slave,' as you call yourself! But how do you do? I see I have frightened\")\n",
        "print(line_len) # length of longest line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBD6Xg-L7PzN",
        "outputId": "d2a448cc-da7a-456a-960a-9a9245324237"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTING PARAMETERS"
      ],
      "metadata": {
        "id": "2gtYjXL75pvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "emb_dim = 32 or 128\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "block_size = 4 or 8\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "activation_function = \"ReLU\" or \"Sin\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "random_seed = 96 or 42"
      ],
      "metadata": {
        "id": "gSILj5tFCjOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length: how many words do we take to predict the next one\n",
        "num_layers = 3 # number of layers in the model\n",
        "hidden_size = 1024 # size of the hidden layers in the model\n",
        "emb_dim = 128 # embedding layer dimension\n",
        "batch_size = 2048 # batch size\n",
        "num_epochs = 150 # number of epochs to train for\n",
        "learning_rate = 0.01 # learning rate for the optimizer\n",
        "random_seed = 96 # random seed for reproducibility\n",
        "activation_function = 'Sin' # activation function to use"
      ],
      "metadata": {
        "id": "YNncobyh3wv7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING X AND Y"
      ],
      "metadata": {
        "id": "FrUsI1ZJ5x79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(random_seed) # setting random seed for reproducibility"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c81EwohA7Yjs",
        "outputId": "d6ccf9c2-27f4-4c8c-8f76-25b21c970a62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c8858200dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = [], [] # creating empty lists to store X and Y\n",
        "context = [0]*block_size # creating context list\n",
        "\n",
        "pr_text_split = pr_text.split() # splitting the one big string the text is in into a list of separated words\n",
        "\n",
        "for word in pr_text_split: # iterating through each word\n",
        "  X.append(context) # adding context to X\n",
        "  Y.append(word2int[word]) # adding current word as the one following the previous context\n",
        "  context = context[1:] + [word2int[word]] # updating context to include the word at the end for context for next word"
      ],
      "metadata": {
        "id": "JqFctjhn2sRg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20): # printing context and next word to check\n",
        "  context = X[i]\n",
        "  word = Y[i]\n",
        "  for con in context:\n",
        "    print(int2word[con], end = ' ')\n",
        "  print('--->', end = ' ')\n",
        "  print(int2word[word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3jucgwA5H3W",
        "outputId": "2699adcd-e4a3-45ae-fda5-d0c0d7620a17"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ---> well\n",
            "              well ---> prince\n",
            "            well prince ---> so\n",
            "          well prince so ---> genoa\n",
            "        well prince so genoa ---> and\n",
            "      well prince so genoa and ---> lucca\n",
            "    well prince so genoa and lucca ---> are\n",
            "  well prince so genoa and lucca are ---> now\n",
            "well prince so genoa and lucca are now ---> just\n",
            "prince so genoa and lucca are now just ---> family\n",
            "so genoa and lucca are now just family ---> estates\n",
            "genoa and lucca are now just family estates ---> of\n",
            "and lucca are now just family estates of ---> the\n",
            "lucca are now just family estates of the ---> buonapartes\n",
            "are now just family estates of the buonapartes ---> .\n",
            "now just family estates of the buonapartes . ---> but\n",
            "just family estates of the buonapartes . but ---> i\n",
            "family estates of the buonapartes . but i ---> warn\n",
            "estates of the buonapartes . but i warn ---> you\n",
            "of the buonapartes . but i warn you ---> if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X).to(device) # moving data to GPU\n",
        "Y = torch.tensor(Y).to(device)"
      ],
      "metadata": {
        "id": "LMT4gR4559oQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X:', X.shape)\n",
        "print('Datatype of X:', X.dtype)\n",
        "print('Shape of Y:', Y.shape)\n",
        "print('Datatype of Y:', Y.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U-hqvn96AWW",
        "outputId": "7e52ad8d-e64b-4937-d6c6-16b8a4b1b704"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: torch.Size([593963, 8])\n",
            "Datatype of X: torch.int64\n",
            "Shape of Y: torch.Size([593963])\n",
            "Datatype of Y: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING EMBEDDING LAYER"
      ],
      "metadata": {
        "id": "fLnQ_J6A6c4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an embedding layer basically means assigning a certain real valued vector with emb_dim dimensions to each unique word in the vocabulary. This reduces the number of dimensions from what we might have used for each unique word had we used one-hot encoding. This transforms the discrete vocabulary data into continuous data.\n",
        "\n",
        "A dense vector is a type of vector where most (or all) of its elements are non-zero. It is the opposite of a sparse vector, where most elements are zero."
      ],
      "metadata": {
        "id": "Fuywvt666kIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb = torch.nn.Embedding(len(word2int), emb_dim) # embedding layer for words"
      ],
      "metadata": {
        "id": "AlGn9CqI6auz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb.weight # randomly generated"
      ],
      "metadata": {
        "id": "OhViTbn37eNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431415c2-81b0-4ec6-ba0c-bacef101e296"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 2.4603,  1.2647,  0.2663,  ...,  1.1148, -0.1940,  0.0418],\n",
              "        [-1.1276,  0.0920,  1.1525,  ..., -0.5578, -0.4559, -0.4152],\n",
              "        [-1.5283, -2.0629, -0.3219,  ..., -0.9540,  1.5494, -1.5271],\n",
              "        ...,\n",
              "        [ 0.5906,  0.0234,  0.7349,  ..., -0.5650, -0.7422,  1.2607],\n",
              "        [-1.4616, -0.5968,  0.9015,  ..., -1.3212, -0.1003, -0.1985],\n",
              "        [-0.3401,  1.8790, -0.5124,  ..., -0.8150, -1.0105,  0.2686]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb.weight.shape # each unique word in the vocabulary has a particular vector"
      ],
      "metadata": {
        "id": "1JiIF1T77fiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abef144-cb34-4a52-8a72-bdcf003637e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18336, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING NEURAL NETWORK MODEL"
      ],
      "metadata": {
        "id": "alUfRPyp7l8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates a neural network model with one input layer which has (block_size * emb_dim) neurons to store the context as vectors, (num_layers) layers with (hidden_size) neurons each and an output layer with (vocab_size) neurons, each corresponding to a unique word in the vocabulary."
      ],
      "metadata": {
        "id": "SiSJ6Pi47ptw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sine_init(m): # SIREN initialization for gradient stability when working with sin activation function\n",
        "    if isinstance(m, nn.Linear):\n",
        "        input_dim = m.weight.size(1)\n",
        "        std = 1 / input_dim  # scale factor for stability\n",
        "        nn.init.uniform_(m.weight, -std, std)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "class NextWord(nn.Module):\n",
        "  def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation_function,num_layers=3): # init method defines the architecture of the neural network\n",
        "    super().__init__() # calls the superclass and its constructor\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim) # embedding layer\n",
        "    self.layers = nn.ModuleList() # list of layers\n",
        "    hl_1 = nn.Linear(block_size * emb_dim, hidden_size)\n",
        "    if activation_function == 'Sin':\n",
        "      sine_init(hl_1)\n",
        "    self.layers.append(hl_1) # first layer, maps from (block_size * emb_dim) neurons to (hidden_size) neurons\n",
        "    for i in range(num_layers): # creating hidden layers\n",
        "      layer = nn.Linear(int(hidden_size/(2**i)), int(hidden_size/(2**(i+1))))\n",
        "      if activation_function == 'Sin':\n",
        "        sine_init(layer)\n",
        "      self.layers.append(layer)\n",
        "    self.layers.append(nn.Linear(int(hidden_size/(2**num_layers)), vocab_size)) # output layer\n",
        "    if activation_function == 'ReLU':\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation_function == 'Sin':\n",
        "      self.activation = lambda x: torch.sin(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.emb(x) # embedding layer\n",
        "    x = x.view(x.shape[0], -1) # flatten the embedding layer\n",
        "    for layer in self.layers[:-1]: # passing through the layers\n",
        "      x = layer(x)\n",
        "      x = self.activation(x)\n",
        "    x= self.layers[-1](x) # output layer\n",
        "    return x"
      ],
      "metadata": {
        "id": "dGtPt8HF7ilX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NextWord(block_size, vocab_size, emb_dim, hidden_size,activation_function).to(device) # creating the model\n",
        "# model = torch.compile(model) # compiling the model\n",
        "loss_fn = nn.CrossEntropyLoss() # loss function\n",
        "opt = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.65) # optimizer"
      ],
      "metadata": {
        "id": "-CSDzXKv7xMm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param_name, param in model.named_parameters():\n",
        "    print(param_name, param.shape)"
      ],
      "metadata": {
        "id": "CdVx3IrC76It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad22ca1-7ad4-4c15-8179-097d8c229558"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb.weight torch.Size([18336, 128])\n",
            "layers.0.weight torch.Size([1024, 1024])\n",
            "layers.0.bias torch.Size([1024])\n",
            "layers.1.weight torch.Size([512, 1024])\n",
            "layers.1.bias torch.Size([512])\n",
            "layers.2.weight torch.Size([256, 512])\n",
            "layers.2.bias torch.Size([256])\n",
            "layers.3.weight torch.Size([128, 256])\n",
            "layers.3.bias torch.Size([128])\n",
            "layers.4.weight torch.Size([18336, 128])\n",
            "layers.4.bias torch.Size([18336])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATING TEXT FROM UNTRAINED MODEL"
      ],
      "metadata": {
        "id": "BPKnclE47-LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_in_lines(text, line_len, number_lines):\n",
        "    text_words = text.split()  # splitting the text into words\n",
        "    on_word = 0  # index to iterate through words\n",
        "    for _ in range(number_lines):  # for specified number of lines\n",
        "        curr_len = 0  # length of current line\n",
        "        while on_word < len(text_words) and curr_len + len(text_words[on_word]) <= line_len:\n",
        "            print(text_words[on_word], end=' ')\n",
        "            curr_len += len(text_words[on_word]) + 1  # +1 for the space after the word\n",
        "            on_word += 1\n",
        "        print()  # onto the next line"
      ],
      "metadata": {
        "id": "PtavNFG18CcY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_context(input_text):\n",
        "  input_words = input_text.split()\n",
        "  if len(input_words) < block_size: # context has to be a minimum length, if the input provided isn't long enough, padding it\n",
        "    input_words = [0] * (block_size - len(input_words)) + input_words\n",
        "  words_for_context = input_words[len(input_words)-block_size:len(input_words)] # 0 or more, to take the last (block_size) words for context\n",
        "  context  = []\n",
        "  for word in words_for_context:\n",
        "    if word in word2int.keys():\n",
        "      context.append(word2int[word])\n",
        "    else:\n",
        "      context.append(18335)\n",
        "  context = [word2int[word] for word in words_for_context] # mapping the words to ints\n",
        "  return context"
      ],
      "metadata": {
        "id": "QDVH3ouq8XvD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_context = [0]*block_size # defining a base case for context in case of empty input"
      ],
      "metadata": {
        "id": "YSEh0mZ9_Paq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, int2word, word2int, block_size, k_lines = 10, context = base_context):\n",
        "    gen_text = '' # initializing output text\n",
        "    max_len = line_len*k_lines + max_word_len # deciding number of words to be predicted for printing k lines\n",
        "    text_len = 0 # initializing number of words in output text\n",
        "    while text_len < max_len: # predicting words one by one\n",
        "      x = torch.tensor(context).view(1, -1).to(device) # converting context to tensor, flattening it, saving it to the gpu\n",
        "      y_pred = model(x) # predicting next word\n",
        "      ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item() # getting int for next word\n",
        "      wor = int2word[ix] # mapping int to word\n",
        "      gen_text += wor + ' ' # adding word to output with space\n",
        "      context = context[1:] + [ix] # updating context\n",
        "      text_len += 1 + len(wor) # updating output length\n",
        "\n",
        "    return gen_text"
      ],
      "metadata": {
        "id": "rR8wsksy_S12"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_in_lines(generate_text(model, int2word, word2int, block_size, 10), line_len, 10)"
      ],
      "metadata": {
        "id": "WKnWqV0N8RML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7a87aa-dcda-4d36-8e0f-5f652aacadfe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flatly pimples eggshell torch expedient 1809 brisk ask relevant liberty \n",
            "tremor barefooted assistance oaks repletion trammeled yellow spangled \n",
            "fardistant desperately efficacy lelorgne platons mistook mesdames \n",
            "roadside galloped summit bestknown hating phenomena strikes days \n",
            "artificially boss 178 tavern stream fir pranced refutation mysterious \n",
            "accusations acid sour contributions disinclined group dressmakers aloof \n",
            "treated predilection batard 62 bullet helplessness report brows chucked \n",
            "stirring easy relentless coral castanets agent truthful caldrons \n",
            "starshaped wicket woodwork beatified doubling confined charms remotest \n",
            "obolenski majordomo correctly recently elusive clement duets echkino \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING MODEL"
      ],
      "metadata": {
        "id": "lPI3-7in_cQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mini-batch training\n",
        "\n",
        "print_every = 10 # printing loss every 10 epochs\n",
        "elapsed_time = []\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i+batch_size]\n",
        "        y = Y[i:i+batch_size]\n",
        "        y_pred = model(x)\n",
        "        x=torch.where((x>18335)|(x<0), torch.tensor(18335).to(device), x)\n",
        "\n",
        "        loss = loss_fn(y_pred, y) # calulating loss\n",
        "        loss.backward() # backpropagation\n",
        "        opt.step() # updating weights\n",
        "        opt.zero_grad() # clears model parameters beofre next iteration\n",
        "    end_time = time.time()\n",
        "    elapsed_time.append(end_time - start_time)\n",
        "    if epoch % print_every == 0 or epoch==num_epochs-1:\n",
        "        print(epoch, loss.item()) # printing loss value to keep track of the process"
      ],
      "metadata": {
        "id": "qIXLfEbJ_Z68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29b79f3-754f-4041-8a74-89a218edd19c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 9.675047874450684\n",
            "10 6.735415458679199\n",
            "20 6.440263748168945\n",
            "30 6.197668552398682\n",
            "40 6.063780307769775\n",
            "50 5.974270343780518\n",
            "60 5.895053386688232\n",
            "70 5.786025047302246\n",
            "80 5.6757025718688965\n",
            "90 5.595744609832764\n",
            "100 5.54191780090332\n",
            "110 5.500536918640137\n",
            "120 5.383577823638916\n",
            "130 5.295692443847656\n",
            "140 5.238498687744141\n",
            "149 5.186456680297852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = f'model_{emb_dim}_{block_size}_{activation_function}_{random_seed}.pth' # naming using model parameters\n",
        "torch.save(model.state_dict(), folder_path + model_name)"
      ],
      "metadata": {
        "id": "JRyjqyXm_tk-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE7MrpZGYL_U",
        "outputId": "c23e369c-9835-45bf-fd54-782b2e959353"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_128_8_Sin_96.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATING TEXT FROM TRANIED MODEL"
      ],
      "metadata": {
        "id": "o3ozlfDz_p17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_in_lines(generate_text(model, int2word, word2int, block_size, 10), line_len, 10)"
      ],
      "metadata": {
        "id": "GGbDn6ZV_u_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5993b08-4e43-4827-ddb5-2f4cb9f25241"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". another lisa . out cruel they mounted in a law arms more two saw any \n",
            "go she have had rose only room the kissed of be much plumes not the 26 \n",
            "on order the fresh ekonomov and at one dokhturovs arm demanded and mary \n",
            "when her own putting say of tip now an cause and that which in the rein \n",
            "of eyes during princess in the wifes of sixth had ever please the \n",
            "ceremony seemed now that news in whom began his war became youll to the \n",
            "greatest throat and monsieur blushed he one in helene had of naval to \n",
            "grass anna daughters hand for rostovs had letter about staff himself \n",
            "there go what the faced dron impeding help . to way that river so . \n",
            "produced understood . confidently thing for then the he of drawing i \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyQgpjdaiGxG"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}